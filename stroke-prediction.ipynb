{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Stroke prediction dataset: EDA + ML Classification models </h1>","metadata":{}},{"cell_type":"markdown","source":"<h2>Table of contents</h2>\n\n* [Introduction](#Introduction)\n    - [Personal motivation](#Personal-motivation)\n    - [Objectives](#Objectives)\n* [Importing](#Importing)\n* [EDA (Exploratory Data Analysis](#EDA)\n    - [Overview](#Overview)\n    - [Univariate analysis](#univariate)\n        + [Categorical variables](#Categorical-uni)\n        + [Numerical variables](#Numerical-uni)\n    - [Correlations](#Correlations)\n    - [Bivariate analysis](#Bivariate)\n        + [Categorical variables](#Categorical-bi)\n        + [Numerical variables](#Numerical-bi)\n* [Modeling](#Models)\n    - [SMOTE](#Smote)\n    - [Applying the model and cross-validation](#Applying)\n    - [Hyperparameter optimization](#Hyperparam-tuning)\n    - [Classifying my own data](#Myself)\n    - [ROC Curve and confusion matrix](#ROC)\n* [Conclusions](#Conclusions)","metadata":{}},{"cell_type":"markdown","source":"<h2>1.- Introduction</h2>\n<a id=\"Introduction\"></a>\n\n<p>Stroke is the 2nd leading cause of death wordwide, only behind heart attacks, responsible for approximately 11% of total deaths, according to the World Health <br/> Organization Global Estimates. Taking into account that is a preventable condition, much effort has been put into detecting its causes. In this analysis we will try to <br/>understand, with the available data, what kind of relationship exists between a stroke event and other physiological and enviromental factors, such as age, glucose levels and smoking habits. <br/>\n</p>\n<br/>\n<h3>1.1.- Personal motivation</h3>\n<a id=\"Personal-motivation\"></a>\n<p>Stroke events are really common in my family. None of the cases were lethal, but three out of four of my grandparents and also my father had one, both ischemic</br> (lack of blood flow in the brain due to a blood clot) and hemorrhagic (bleeding in the brain because of damaged blood vessels), with different levels</br> of consecuences after, ranging from none at all to severe motor and language skills impairement. Therefore, i thought it could be intereseting to see what </br>\na machine learning classification algorithm trained with this data has to say about my own condition, using it to classify my own data. </p>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<h3>1.2.- Objectives</h3>\n<a id=\"Objectives\"></a>\n<ul>\n<li>Describe the distribution of the numerical variables such as age and body mass index and how they can affect the probability of stroke.</li>\n<li>Analyze the characteristics of categorical variables and its relationship with stroke.</li>\n<li>Implement machine learning classification algorithms to predict stroke, select the best performing ones and try to optimize it.</li>\n<li>Observe what is the predicted class by the model with my own data (wish me luck).</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<h2>2.- Importing libraries and loading dataset</h2> \n<a id=\"Importing\"></a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:43.791852Z","iopub.execute_input":"2023-01-25T12:36:43.792319Z","iopub.status.idle":"2023-01-25T12:36:44.467898Z","shell.execute_reply.started":"2023-01-25T12:36:43.792228Z","shell.execute_reply":"2023-01-25T12:36:44.466668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly \nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.subplots import make_subplots\n\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:44.470073Z","iopub.execute_input":"2023-01-25T12:36:44.470483Z","iopub.status.idle":"2023-01-25T12:36:46.651353Z","shell.execute_reply.started":"2023-01-25T12:36:44.470448Z","shell.execute_reply":"2023-01-25T12:36:46.650412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CUR_DIR = Path('__file__').resolve().parent\nPAR_DIR = CUR_DIR.parent\n\n\ndataframe = pd.read_csv(f'{PAR_DIR}/input/stroke/healthcare-dataset-stroke-data.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:46.653062Z","iopub.execute_input":"2023-01-25T12:36:46.654230Z","iopub.status.idle":"2023-01-25T12:36:46.694119Z","shell.execute_reply.started":"2023-01-25T12:36:46.654184Z","shell.execute_reply":"2023-01-25T12:36:46.693074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>3.- Exploratory Data Analysis</h2>\n<a id=\"EDA\"></a>","metadata":{}},{"cell_type":"markdown","source":"<h3>3.1.- Overview of the dataset</h3>\n<a id=\"Overview\"></a>","metadata":{}},{"cell_type":"code","source":"dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:46.696474Z","iopub.execute_input":"2023-01-25T12:36:46.696845Z","iopub.status.idle":"2023-01-25T12:36:46.724526Z","shell.execute_reply.started":"2023-01-25T12:36:46.696812Z","shell.execute_reply":"2023-01-25T12:36:46.723591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe.info()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:46.725965Z","iopub.execute_input":"2023-01-25T12:36:46.726339Z","iopub.status.idle":"2023-01-25T12:36:46.757138Z","shell.execute_reply.started":"2023-01-25T12:36:46.726305Z","shell.execute_reply":"2023-01-25T12:36:46.755773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:46.758547Z","iopub.execute_input":"2023-01-25T12:36:46.759028Z","iopub.status.idle":"2023-01-25T12:36:46.770812Z","shell.execute_reply.started":"2023-01-25T12:36:46.758986Z","shell.execute_reply":"2023-01-25T12:36:46.769459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><b>Observations</b>: we have some missing values in the BMI column. We will deal with this by replacing those missing values with the mean for that column. </p>","metadata":{}},{"cell_type":"code","source":"dataframe.bmi.replace(to_replace=np.nan, value=dataframe.bmi.mean(), inplace=True)\ndataframe.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:46.772409Z","iopub.execute_input":"2023-01-25T12:36:46.772862Z","iopub.status.idle":"2023-01-25T12:36:46.787600Z","shell.execute_reply.started":"2023-01-25T12:36:46.772824Z","shell.execute_reply":"2023-01-25T12:36:46.786259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical = ['gender', 'hypertension', 'heart_disease', 'ever_married',\n'work_type', 'Residence_type', 'smoking_status']\n\nnumerical = ['age','avg_glucose_level', 'bmi']\ndataframe[numerical].describe()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:46.789016Z","iopub.execute_input":"2023-01-25T12:36:46.789513Z","iopub.status.idle":"2023-01-25T12:36:46.819684Z","shell.execute_reply.started":"2023-01-25T12:36:46.789478Z","shell.execute_reply":"2023-01-25T12:36:46.818443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>3.2.- Univariate analysis</h2>\n<a id=\"Univariate\"></a>","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nsns.set_style(style='whitegrid')\nsns.set_context(context='notebook')\nplt.rcParams['figure.figsize'] = (11, 9.4)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:46.821576Z","iopub.execute_input":"2023-01-25T12:36:46.822200Z","iopub.status.idle":"2023-01-25T12:36:46.832898Z","shell.execute_reply.started":"2023-01-25T12:36:46.822153Z","shell.execute_reply":"2023-01-25T12:36:46.831409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(9,7))\nbars = ax.bar(dataframe['stroke'].value_counts().index, height = dataframe['stroke'].value_counts().values)\nplt.title('Proportion Of Stroke Samples')\nax.bar_label(bars)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:46.839214Z","iopub.execute_input":"2023-01-25T12:36:46.839778Z","iopub.status.idle":"2023-01-25T12:36:47.161955Z","shell.execute_reply.started":"2023-01-25T12:36:46.839744Z","shell.execute_reply":"2023-01-25T12:36:47.160756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><b>Observations</b>: the dataset has a significant inbalance with few positive cases in the stroke column. </br> \nGiven that this is the variable that we want to mode, we will have to deal with this somehow when applying the ML models.</p>","metadata":{}},{"cell_type":"markdown","source":"<h3>3.2.a.- Categorical variables</h3>\n<a id=\"Categorical-uni\"></a>","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=7, ncols=1,figsize=(7,25), squeeze=False)\nfor i in range(len(categorical)):\n    bars = ax[i,0].barh(dataframe.value_counts(categorical[i]).index, width = dataframe.value_counts(categorical[i]).values)\n    ax[i,0].bar_label(bars)\n    ax[i,0].set_title(categorical[i])\nfig.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:47.163466Z","iopub.execute_input":"2023-01-25T12:36:47.163921Z","iopub.status.idle":"2023-01-25T12:36:48.946839Z","shell.execute_reply.started":"2023-01-25T12:36:47.163876Z","shell.execute_reply":"2023-01-25T12:36:48.944767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><b>Observations</b>: the gender column has only 1 value in the \"other\" category. </p>","metadata":{}},{"cell_type":"markdown","source":"<h3>3.2.b.- Numerical variables</h3>\n<a id=\"Numerical-uni\"></a>","metadata":{}},{"cell_type":"code","source":"dataframe[numerical].hist(figsize=(9,7))","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:48.948183Z","iopub.execute_input":"2023-01-25T12:36:48.948553Z","iopub.status.idle":"2023-01-25T12:36:49.524207Z","shell.execute_reply.started":"2023-01-25T12:36:48.948521Z","shell.execute_reply":"2023-01-25T12:36:49.522758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>3.2.c.- Correlations</h3>\n<a id=\"Correlations\"></a>","metadata":{}},{"cell_type":"code","source":"dataframe.corr()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:49.525809Z","iopub.execute_input":"2023-01-25T12:36:49.526223Z","iopub.status.idle":"2023-01-25T12:36:49.550246Z","shell.execute_reply.started":"2023-01-25T12:36:49.526187Z","shell.execute_reply":"2023-01-25T12:36:49.548452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe1 = dataframe.drop('id', axis=1)\nsns.heatmap(dataframe1.corr(), cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:49.551644Z","iopub.execute_input":"2023-01-25T12:36:49.552083Z","iopub.status.idle":"2023-01-25T12:36:49.897278Z","shell.execute_reply.started":"2023-01-25T12:36:49.552051Z","shell.execute_reply":"2023-01-25T12:36:49.895542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><b>Observations</b>: the stronger correlation for the stroke variable seems to be with the age variable (Pearson coef = 0.25)</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>3.3.- Bivariate analysis</h2>\n<a id=\"Bivariate\"></a>","metadata":{}},{"cell_type":"markdown","source":"<h3>3.3.a.- Categorical variables</h3>\n<a id=\"Categorical-bi\"></a>","metadata":{}},{"cell_type":"code","source":"init_notebook_mode(connected=True)\nfor i in range(len(categorical)):\n    fig = px.histogram(dataframe, x=categorical[i], color=\"stroke\", title= f'{categorical[i]} & stroke condition', width=400, height=400)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:49.898901Z","iopub.execute_input":"2023-01-25T12:36:49.899271Z","iopub.status.idle":"2023-01-25T12:36:51.361696Z","shell.execute_reply.started":"2023-01-25T12:36:49.899241Z","shell.execute_reply":"2023-01-25T12:36:51.360402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>3.3.b.- Numerical variables</h3>\n<a id=\"Numerical-bi\"></a>","metadata":{}},{"cell_type":"code","source":"init_notebook_mode(connected=True)\nfig = px.scatter(dataframe, x='age', y='bmi', title='Age & BMI ', color='stroke', hover_data = dataframe[['stroke']])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:51.363419Z","iopub.execute_input":"2023-01-25T12:36:51.363906Z","iopub.status.idle":"2023-01-25T12:36:51.507769Z","shell.execute_reply.started":"2023-01-25T12:36:51.363861Z","shell.execute_reply":"2023-01-25T12:36:51.506371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"init_notebook_mode(connected=True)\nfig = px.scatter(dataframe, x='age', y='avg_glucose_level', title='Age & Average Glucose Level ', color='stroke', hover_data = dataframe[['stroke']])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:51.509606Z","iopub.execute_input":"2023-01-25T12:36:51.510070Z","iopub.status.idle":"2023-01-25T12:36:51.628745Z","shell.execute_reply.started":"2023-01-25T12:36:51.510027Z","shell.execute_reply":"2023-01-25T12:36:51.627433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><b>Observations</b>: there are more positive stroke cases in the upper right quadrant of the plot (higher values of glucose level and older age).</p>","metadata":{}},{"cell_type":"markdown","source":"<h2>4.- Machine learning models</h2>\n<a id=\"Models\"></a>","metadata":{}},{"cell_type":"markdown","source":"<h3>4.1.- Importing libraries</h3> ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:51.630336Z","iopub.execute_input":"2023-01-25T12:36:51.630799Z","iopub.status.idle":"2023-01-25T12:36:52.259241Z","shell.execute_reply.started":"2023-01-25T12:36:51.630756Z","shell.execute_reply":"2023-01-25T12:36:52.257465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe[0:10]","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:52.261620Z","iopub.execute_input":"2023-01-25T12:36:52.262161Z","iopub.status.idle":"2023-01-25T12:36:52.285129Z","shell.execute_reply.started":"2023-01-25T12:36:52.262110Z","shell.execute_reply":"2023-01-25T12:36:52.283591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>We will first convert the categorical columns to numbers, because typically the ML algorithms work better with this type of data.</p>","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ndf_labeled = dataframe.apply(le.fit_transform)\ndf_labeled[0:10]","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:52.287039Z","iopub.execute_input":"2023-01-25T12:36:52.287571Z","iopub.status.idle":"2023-01-25T12:36:52.318769Z","shell.execute_reply.started":"2023-01-25T12:36:52.287523Z","shell.execute_reply":"2023-01-25T12:36:52.317528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_labeled.drop(['stroke'], axis=1)\ny = df_labeled.stroke\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=15)\n\nprint(f'Number of positive stroke total cases: {sum(y==1)}')\nprint(f'Number of negative stroke total cases: {sum(y==0)}')\nprint(f'Number of positive stroke train cases: {sum(y_train==1)}')\nprint(f'Number of negative stroke train cases: {sum(y_train==0)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:52.320049Z","iopub.execute_input":"2023-01-25T12:36:52.320444Z","iopub.status.idle":"2023-01-25T12:36:52.339083Z","shell.execute_reply.started":"2023-01-25T12:36:52.320403Z","shell.execute_reply":"2023-01-25T12:36:52.336985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>4.2.- SMOTE</h3>\n<a id=\"Smote\"></a>\n<p>As observed in section 3.2, our data is severely undersampled for the \"stroke positive\" class. Given that this is precisely the most importan class for the <br/>\nclassification, we need to correct this imbalance in order to obtain good results from our ML algorithms. The SMOTE technique is a possible solution for this particular problem.</p>\n<img src='https://i.ytimg.com/vi/adHqzek--d0/maxresdefault.jpg' width=\"800\" height=\"600\"></img>","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nX = df_labeled.drop(['stroke'], axis=1)\ny = df_labeled.stroke\nsm = SMOTE()\nX_res, y_res = sm.fit_resample(X,y)\nx_train,x_test,y_train,y_test = train_test_split(X_res,y_res, test_size=0.3,random_state=15)\n\nprint(f'Number of positive stroke total cases: {sum(y_res==1)}')\nprint(f'Number of negative stroke total cases: {sum(y_res==0)}')\nprint(f'Number of positive stroke train cases: {sum(y_train==1)}')\nprint(f'Number of negative stroke train cases: {sum(y_train==0)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:52.340478Z","iopub.execute_input":"2023-01-25T12:36:52.340824Z","iopub.status.idle":"2023-01-25T12:36:52.514281Z","shell.execute_reply.started":"2023-01-25T12:36:52.340793Z","shell.execute_reply":"2023-01-25T12:36:52.512966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>4.3.- Defininig and applying our classification models</h3>\n<a id=\"Applying\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"\n<h4>K-fold crossvalidation method<h4>\n<p>After defining and fitting our models we will use de K-Fold strategy for cross-validation, wich splits our trainig set into <br/>\nk number of sub-sets or <i>folds</i> and will use a different one as a validation set each iteration.</p>\n<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"800\" height=\"600\"></img>","metadata":{}},{"cell_type":"code","source":"models = []\nmodels.append(['Logistic Regreesion', LogisticRegression(random_state=11)])\nmodels.append(['KNNeighbors', KNeighborsClassifier()])\nmodels.append(['Decision Tree', DecisionTreeClassifier(random_state=11)])\nmodels.append(['Random Forest', RandomForestClassifier(random_state=11)]) #Bagging based model\nmodels.append(['Ada Boost ',AdaBoostClassifier()]) #Boosting based model\nmodels.append(['XGBoost', XGBClassifier()])  #Boosting based model\n\ndef run_models(x_train, x_test, y_train, y_test):\n    models_score = []\n    kf_score = []\n    for name,model in models:\n        model = model\n        model.fit(x_train,y_train)\n        model_pred = model.predict(x_test)\n        score = accuracy_score(y_test,model.predict(x_test))\n        models_score.append(score)\n        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=11)\n        cv_score = np.mean(cross_val_score(estimator = model, X = x_train, y = y_train, cv = cv)) \n        kf_score.append(cv_score)\n        print(f'Accuracy Score for {name} model: {score}')\n        print(f'Accuracy K-Fold Crossvalidation Mean Score for {name} model: {cv_score}')\n        print('#'*25)\n    return models_score, kf_score\n\nmodels_score, kf_score = run_models(x_train,x_test,y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:36:52.515547Z","iopub.execute_input":"2023-01-25T12:36:52.515871Z","iopub.status.idle":"2023-01-25T12:38:02.679082Z","shell.execute_reply.started":"2023-01-25T12:36:52.515841Z","shell.execute_reply":"2023-01-25T12:38:02.677971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_names = [\n'Logistic Regression',\n'K-Nearest Neighbor',\n'Decision Tree Classifier',\n'Random Forest Classifier',\n'Ada Boost',\n'XG Boost']\n\nplt.rcParams['figure.figsize'] = (14, 8)\nax = sns.barplot(x=models_names, y=models_score, palette = \"mako\")\nplt.title('Accuracy of Classifier Models implemented')\nplt.xlabel('Model' )\nplt.ylabel('Accuracy')\nax.bar_label(ax.containers[0])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:38:02.683552Z","iopub.execute_input":"2023-01-25T12:38:02.686060Z","iopub.status.idle":"2023-01-25T12:38:02.992793Z","shell.execute_reply.started":"2023-01-25T12:38:02.686005Z","shell.execute_reply":"2023-01-25T12:38:02.991811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><b>Observation:</b> XGBoost and Random Forest are the best performing models according to the model accuracy obtained. <br/>\nWe will take the best (XGBoost) and worst (Logistic regression) performing models and see if we can obtain a better performance after hyperparameter tuning.</p>","metadata":{}},{"cell_type":"markdown","source":"<h3>4.4.- Hyperparameter Optimization</h3>\n<a id=\"Hyperparam-tuning\"></a>","metadata":{}},{"cell_type":"markdown","source":"<p>Grid-search is a method used to find the optimal hyperparameters of a model. It searches for the best combination of hyperparameters</br>\nbetween all possible combinations of values that we want to try. In this case: {'C':[0.25,0.5,0.75,1]} for the logistic regression and </br>\n{'max_depth': [3, 5, 7, 9],  'n_estimators': [5, 10, 15, 20, 25, 50, 100], 'learning_rate': [0.01, 0.05, 0.1]} for the XGBoost</p>\n<img src=\"https://miro.medium.com/max/1000/1*9W1MrRkHi0YFmBoHi9Y2Ow.png\" width = 500 height = 250>","metadata":{}},{"cell_type":"code","source":"def tune_model(x_train, y_train):\n    grid_models = [(LogisticRegression(),[{'C':[0.25,0.5,0.75,1],'random_state':[11]}]), \n              (XGBClassifier(), [{'max_depth': [3, 5, 7, 9],  'n_estimators': [5, 10, 15, 20, 25, 50, 100], 'learning_rate': [0.01, 0.05, 0.1]}])]\n    tuned_params = []\n    i = 0\n    for model,params in grid_models:\n        grid = GridSearchCV(estimator=model,param_grid = params, scoring = 'accuracy',cv = 5)\n        grid.fit(x_train, y_train)\n        best_accuracy = grid.best_score_\n        best_params = grid.best_params_\n        tuned_params.append(best_params)\n        if i == 0:\n            print(f'Best accuracy for Logistic Regression model: {best_accuracy}') \n            print(f'Best parameters found for Logistic Regression model: {best_params}')\n            i += 1\n        else:\n            print(f'Best accuracy for XGBoost model: {best_accuracy}')\n            print(f'Best parameters found for XGBoost model: {best_params}')\n    return tuned_params\ntune = tune_model(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:38:02.994205Z","iopub.execute_input":"2023-01-25T12:38:02.994886Z","iopub.status.idle":"2023-01-25T12:39:57.508301Z","shell.execute_reply.started":"2023-01-25T12:38:02.994848Z","shell.execute_reply":"2023-01-25T12:39:57.506737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>4.5.- ROC Curve and confusion matrix for best model</h3>\n<a id=\"ROC\"></a>","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier(learning_rate = 0.1, max_depth = 9, n_estimators = 100)\nmodel.fit(x_train,y_train)\nmodel_pred = model.predict(x_test)\ncm_xgb = confusion_matrix(y_test, model_pred)\ny_prob = model.predict_proba(x_test)[:,1]\n\nfpr, tpr, thresh = roc_curve(y_test, y_prob)\n\nprint(cm_xgb)\n\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm_xgb, annot=True, fmt = 'd', linewidths = 5, cmap=\"mako\", cbar = False, annot_kws = {'fontsize': 13},\n             yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.title('Confusion Matrix for XGBoost model')\nplt.show()\n\ninit_notebook_mode(connected=True)\nfig = px.area(\n    x=fpr, y=tpr,\n    title=f'ROC Curve (AUC={auc(fpr, tpr):.3f})',\n    labels={'x':'False Positive Rate', 'y':'True Positive Rate'},\n    width=665, height=400\n)\nfig.add_shape(\n    type='line', line={'dash':'dash'},\n    x0=0, x1=1, y0=0, y1=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:39:57.510516Z","iopub.execute_input":"2023-01-25T12:39:57.511459Z","iopub.status.idle":"2023-01-25T12:39:58.834569Z","shell.execute_reply.started":"2023-01-25T12:39:57.511402Z","shell.execute_reply":"2023-01-25T12:39:58.833191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>4.6.- Classifying my data</h3>\n<a id=\"Myself\"></a>","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier(learning_rate = 0.1, max_depth = 9, n_estimators = 100)\nmodel.fit(x_train,y_train)\nmodel_pred = model.predict(x_test)\ncm_xgb = confusion_matrix(y_test, model_pred)\ny_prob = model.predict_proba(x_test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:39:58.836074Z","iopub.execute_input":"2023-01-25T12:39:58.836483Z","iopub.status.idle":"2023-01-25T12:39:59.955793Z","shell.execute_reply.started":"2023-01-25T12:39:58.836446Z","shell.execute_reply":"2023-01-25T12:39:59.954797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.loc[len(x_test.index)] = [6000, 1, 30, 0, 0, 0, 3, 1, 75, 17, 2] #Appending my own data to the test dataset\nx_test.tail()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:39:59.962090Z","iopub.execute_input":"2023-01-25T12:39:59.963001Z","iopub.status.idle":"2023-01-25T12:39:59.982284Z","shell.execute_reply.started":"2023-01-25T12:39:59.962950Z","shell.execute_reply":"2023-01-25T12:39:59.981050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.loc[x_test['id'] == 6000]","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:39:59.985548Z","iopub.execute_input":"2023-01-25T12:39:59.986770Z","iopub.status.idle":"2023-01-25T12:40:00.002208Z","shell.execute_reply.started":"2023-01-25T12:39:59.986710Z","shell.execute_reply":"2023-01-25T12:40:00.000818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test.loc[x_test['id'] == 6000])\ny_prob = model.predict_proba(x_test.loc[x_test['id'] == 6000])\nprint(f'The predicted category for the stroke class using the XGBoost model is {y_pred[0]} with a {y_prob[0, 0]*100}% probability')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:40:00.004968Z","iopub.execute_input":"2023-01-25T12:40:00.006077Z","iopub.status.idle":"2023-01-25T12:40:00.030258Z","shell.execute_reply.started":"2023-01-25T12:40:00.006023Z","shell.execute_reply":"2023-01-25T12:40:00.028958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>5.- Conclusions</h2>\n<a id=\"Conclusions\"></a>\n<ul>\n<li>Age seems to be the most correlated variable with stroke class for the numerical variables, followed by glucose level.</li>\n<li>At first glance it doesn't seem to exist an obvious pattern between smoking status and stroke class, surprisingly, altough we only perform a <br/>\nfirst look descriptive analysis of the categorical variables.</li>\n<li>The best performing classification model with this dataset was the XGBoost Classifier.</li>\n<li>The accuracy of the model was lower <b>after</b> the hyperparameter optimization compared to before. This may be caused by a low coverage of the <br/>\nhyperparameter dimension, i.e, we didn't try enough combinations of hyperparameters to found the optimal ones. A more exhaustive grid search, with more combinations<br/>\ncan be a solution to this problem, but this requires more running time.</li>\n<li>I was classified by the model as a non-stroke class. Of course, this doesn't replace a doctor's opinion, wich i already have, but it was fun to try!</li>\n</ul>","metadata":{}}]}